# Weather Data Pipeline (Ukraine)

## üìå Project Overview

This project is an **end-to-end data engineering pipeline** that collects weather data for Ukrainian cities, stores raw data in MongoDB, transforms it, and loads curated data into PostgreSQL. The pipeline is orchestrated with **Apache Airflow** and runs on an hourly schedule.

The project is designed as a **pet project for a strong junior data engineer**, demonstrating core data engineering concepts: layering, idempotency, data quality checks, and orchestration.

---

## üèó Architecture

```
OpenWeather API
        |
        v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Airflow DAG     ‚îÇ  (hourly)
‚îÇ  weather_pipeline‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        |
        v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MongoDB (RAW)    ‚îÇ -----> ‚îÇ PostgreSQL (STAGING)        ‚îÇ
‚îÇ weather_raw      ‚îÇ        ‚îÇ staging.stg_weather_obs    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Layers

* **RAW (MongoDB)**
  Stores unmodified API responses with ingestion metadata.

* **STAGING (PostgreSQL)**
  Stores cleaned and validated weather observations with strict schema and constraints.

---

## üß∞ Tech Stack

* **Python 3.12**
* **Apache Airflow** (orchestration)
* **MongoDB** (raw data storage)
* **PostgreSQL** (staging / analytical storage)
* **psycopg2**, **pymongo**, **requests**
* **WSL (Ubuntu 24.04)** for local development

---

## üìÇ Project Structure

```
weather_pipeline/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ weather_pipeline/
‚îÇ       ‚îú‚îÄ‚îÄ weather_pipeline_dag.py
‚îÇ       ‚îú‚îÄ‚îÄ jobs/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ extract_weather.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ load_staging_weather.py
‚îÇ       ‚îú‚îÄ‚îÄ config/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ cities.py
‚îÇ       ‚îî‚îÄ‚îÄ db/
‚îÇ           ‚îú‚îÄ‚îÄ mongo/
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ create_indexes.py
‚îÇ           ‚îî‚îÄ‚îÄ postgres/
‚îÇ               ‚îú‚îÄ‚îÄ 01_create_user.sql
‚îÇ               ‚îú‚îÄ‚îÄ 02_create_database.sql
‚îÇ               ‚îú‚îÄ‚îÄ 03_create_schema.sql
‚îÇ               ‚îú‚îÄ‚îÄ 04_create_tables.sql
‚îÇ               ‚îî‚îÄ‚îÄ 05_create_indexes.sql
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üîÑ Data Flow

1. **Extract**

   * Airflow task calls OpenWeather API
   * Raw response is saved to MongoDB
   * Unique index ensures idempotency

2. **Transform**

   * Raw MongoDB documents are validated and normalized
   * Invalid values are filtered or set to NULL

3. **Load**

   * Clean data is inserted into PostgreSQL staging table
   * `ON CONFLICT DO NOTHING` guarantees safe re-runs

---

## üß™ Data Quality & Idempotency

* MongoDB unique index on:

  ```
  (city_id, observation_ts, source)
  ```

* PostgreSQL constraints:

  * CHECK constraints for ranges
  * NOT NULL on business-critical fields
  * UNIQUE constraint on natural key

These mechanisms allow:

* DAG retries
* Backfills
* Safe manual re-runs

---

## ‚è± Scheduling

* **Schedule**: `@hourly`
* **Catchup**: disabled
* **Retries**: enabled (configurable in DAG)

---

## üîê Configuration

All secrets and connection parameters are provided via environment variables.

Example `.env` file:

```env
OPENWEATHER_API_KEY=your_api_key

POSTGRESQL_USER=weather_user
POSTGRESQL_PASSWORD=secure_password
POSTGRESQL_HOST=localhost
POSTGRESQL_PORT=5432
POSTGRESQL_DB=weather

MONGO_URI=mongodb://localhost:27017
```

> ‚ö†Ô∏è `.env` is not committed to git. Use `.env.example` as a template.

---

## ‚ñ∂Ô∏è How to Run Locally

1. Start MongoDB and PostgreSQL in WSL
2. Activate Python virtual environment
3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```
4. run db/mongo/create_indexes.py to create Mongo collection and indexes
5. run db/postgres/ scripts to create postgres table 01_create_user.sql add -v password='your_password' and run it and 02_create_database.sql from sudo
6. Initialize Airflow:

   ```bash
   airflow standalone
   ```
7. Open Airflow UI and trigger `weather_pipeline_dag`

---

## üöÄ Future Improvements

* Support multiple cities (dimension table)
* Add monitoring and metrics
* Add alerting on failures
* Add data mart / analytical layer
* Containerize with Docker

---

## üë§ Author

**OlehKhyl** ‚Äî aspiring Data Engineer
This project was built as a learning and portfolio project following data engineering best practices.
